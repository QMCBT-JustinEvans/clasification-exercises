{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification:  Acquire data\n",
    "\n",
    "## Goals\n",
    "\n",
    "Data you wish to use in analysis will be stored in a variety of sources. In this lesson, we will review importing data from a csv and via mySQL, and we will also learn how to import data from our local clipboard, a google sheets document, and from an MS Excel file. \n",
    "We will then select one source to use as we continue through the rest of this module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods of Data Acquisition\n",
    "\n",
    "\n",
    "- `read_clipboard`: When you have data copied to your clipboard, you can use pandas to read it into a data frame with `pd.read_clipboard`. This can be useful for quickly transferring data to/from a spreadsheet.   \n",
    "- `read_excel`: This function can be used to create a data frame based on the contents of an Excel spreadsheet.  \n",
    "- `read_csv`: Read from a local csv, or from a the cloud (Google Sheets or AWS S3).    \n",
    "- `read_sql(sql_query, connection_url)`: Read data using a SQL query to a database. You must have the required drivers installed, and a specially formatted url string must be provided.\n",
    "    \n",
    "    ```\n",
    "    # To talk to a mysql database:\n",
    "    python -m pip install pymysql mysql-connector\n",
    "    # the connection url string:\n",
    "    mysql+pymysql://USER:PASSWORD@HOST/DATABASE_NAME\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>read_clipboard:</th>\n",
       "      <th>When</th>\n",
       "      <th>you</th>\n",
       "      <th>have</th>\n",
       "      <th>data</th>\n",
       "      <th>copied</th>\n",
       "      <th>to</th>\n",
       "      <th>your</th>\n",
       "      <th>clipboard,</th>\n",
       "      <th>you.1</th>\n",
       "      <th>...</th>\n",
       "      <th>can.1</th>\n",
       "      <th>be</th>\n",
       "      <th>useful</th>\n",
       "      <th>for</th>\n",
       "      <th>quickly</th>\n",
       "      <th>transferring</th>\n",
       "      <th>data.2</th>\n",
       "      <th>to/from</th>\n",
       "      <th>a.1</th>\n",
       "      <th>spreadsheet.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>read_excel:</td>\n",
       "      <td>This</td>\n",
       "      <td>function</td>\n",
       "      <td>can</td>\n",
       "      <td>be</td>\n",
       "      <td>used</td>\n",
       "      <td>to</td>\n",
       "      <td>create</td>\n",
       "      <td>a</td>\n",
       "      <td>data</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>read_csv:</td>\n",
       "      <td>Read</td>\n",
       "      <td>from</td>\n",
       "      <td>a</td>\n",
       "      <td>local</td>\n",
       "      <td>csv,</td>\n",
       "      <td>or</td>\n",
       "      <td>from</td>\n",
       "      <td>a</td>\n",
       "      <td>the</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>read_sql(sql_query,</td>\n",
       "      <td>connection_url):</td>\n",
       "      <td>Read</td>\n",
       "      <td>data</td>\n",
       "      <td>using</td>\n",
       "      <td>a</td>\n",
       "      <td>SQL</td>\n",
       "      <td>query</td>\n",
       "      <td>to</td>\n",
       "      <td>a</td>\n",
       "      <td>...</td>\n",
       "      <td>string</td>\n",
       "      <td>must</td>\n",
       "      <td>be</td>\n",
       "      <td>provided.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       read_clipboard:              When       you  have   data copied   to  \\\n",
       "0          read_excel:              This  function   can     be   used   to   \n",
       "1            read_csv:              Read      from     a  local   csv,   or   \n",
       "2  read_sql(sql_query,  connection_url):      Read  data  using      a  SQL   \n",
       "\n",
       "     your clipboard, you.1  ...   can.1    be useful        for quickly  \\\n",
       "0  create          a  data  ...    None  None   None       None     NaN   \n",
       "1    from          a   the  ...    None  None   None       None     NaN   \n",
       "2   query         to     a  ...  string  must     be  provided.     NaN   \n",
       "\n",
       "  transferring data.2 to/from a.1 spreadsheet.  \n",
       "0          NaN    NaN     NaN NaN          NaN  \n",
       "1          NaN    NaN     NaN NaN          NaN  \n",
       "2          NaN    NaN     NaN NaN          NaN  \n",
       "\n",
       "[3 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reads data copied to your clipboard\n",
    "df1 = pd.read_clipboard()\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source: A Shared Google Sheet\n",
    "\n",
    "1. Get the shareable link url: https://docs.google.com/spreadsheets/d/BLAHBLAHBLAH/edit#gid=NUMBER\n",
    "\n",
    "2. Turn that into a CSV export URL: Replace `/edit` with `/export`; Add `format=csv` to the beginning of the query string. https://docs.google.com/spreadsheets/d/BLAHBLAHBLAH/export?format=csv&gid=NUMBER:\n",
    "\n",
    "3. Pass it to pd.read_csv, which can take a URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_url = 'https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/edit#gid=341089357'    \n",
    "\n",
    "csv_export_url = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')\n",
    "csv_export_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_googlesheet = pd.read_csv(csv_export_url)\n",
    "df_googlesheet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV (Hosted or Local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flavor</th>\n",
       "      <th>pints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>moolenium crunch</td>\n",
       "      <td>11.057570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bubblegum</td>\n",
       "      <td>6.288724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chubby hubby</td>\n",
       "      <td>7.660815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bubblegum</td>\n",
       "      <td>6.644338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neopolitan</td>\n",
       "      <td>13.600125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             flavor      pints\n",
       "0  moolenium crunch  11.057570\n",
       "1         bubblegum   6.288724\n",
       "2      chubby hubby   7.660815\n",
       "3         bubblegum   6.644338\n",
       "4        neopolitan  13.600125"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the S3 file is private, you will need your S3 configurations setup properly.\n",
    "url = \"https://gist.githubusercontent.com/ryanorsinger/bec2f59a9cef8ae7428cb70b3541354a/raw/ef64298da52e5d70f4d388f5fd48eccdb02ed3f1/ice_cream.csv\"\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source: SQL\n",
    "\n",
    "Create a dataframe from the passengers table in the mySQL database, titanic_db. \n",
    "\n",
    "!!!warning \"Database Credentials\"\n",
    "    It's a bad idea to store your database access credentials (i.e. your username and password) in plaintext in your source code. There are many different ways one could manage secrets like this, but a simple way is to store the values in a python file that is not included along with the rest of your source code. This is what we have done with the `env` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'env' has no attribute 'get_db_url'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_db_url\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitanic_db\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSELECT * FROM passengers\u001b[39m\u001b[38;5;124m'\u001b[39m, url)\n\u001b[1;32m      5\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'env' has no attribute 'get_db_url'"
     ]
    }
   ],
   "source": [
    "url = env.get_db_url('titanic_db')\n",
    "\n",
    "df = pd.read_sql('SELECT * FROM passengers', url)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a function that we can reference later to acquire the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_titanic_data():\n",
    "    url = env.get_db_url('titanic_db')\n",
    "    \n",
    "    return pd.read_sql('SELECT * FROM passengers', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire new data:\n",
    "new_titanic_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll store this function in a file named `acquire.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching Your Data\n",
    "Because data acquisition can take time, it's a common practice to write the data locally to a `.csv` file. \n",
    "\n",
    "1. Do whatever you need to do to produce the dataframe that you need. \n",
    "    - For example `df = pd.read_sql('SELECT * FROM passengers', get_connection('titanic_db'))`\n",
    "    - Or your dataframe could include joins, multiple data sources, etc...\n",
    "    \n",
    "2. Now use `df.to_csv(\"titanic.csv\")` to write that dataframe to the file.\n",
    "3. Now that you've written the csv file, you can use it later in other parts of your pipeline!\n",
    "4. Consider the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_titanic_data():\n",
    "    filename = \"titanic.csv\"\n",
    "    \n",
    "    # if file is available locally, read it\n",
    "    if os.path.isfile(filename):\n",
    "        return pd.read_csv(filename)\n",
    "    \n",
    "    # if file not available locally, acquire data from SQL database\n",
    "    # and write it as csv locally for future use\n",
    "    else:\n",
    "        # read the SQL query into a dataframe\n",
    "        df = new_titanic_data()\n",
    "        \n",
    "        # Write that dataframe to disk for later. Called \"caching\" the data for later.\n",
    "        df.to_csv(filename)\n",
    "\n",
    "        # Return the dataframe to the calling code\n",
    "        return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_titanic_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mget_titanic_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mget_titanic_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(filename)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# if file not available locally, acquire data from SQL database\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# and write it as csv locally for future use\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# read the SQL query into a dataframe\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mnew_titanic_data\u001b[49m()\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Write that dataframe to disk for later. Called \"caching\" the data for later.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(filename)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_titanic_data' is not defined"
     ]
    }
   ],
   "source": [
    "df = get_titanic_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "The end product of these exercise is a jupyter notebook (`classification_exercises.ipynb`) and a `acquire.py` file. The notebook will contain all your work as you move through the exercises. The `acquire.py` file should contain the final functions that acquire the data into a pandas dataframe.\n",
    "\n",
    "1. Make a new repo called `classification-exercises` on both GitHub and within your `codeup-data-science` directory. This will be where you do your work for this module. \n",
    "\n",
    "2. Inside of your local `classification-exercises` repo, create a file named `.gitignore` and list the following file names and paths: `env.py`, `.DS_Store`, `.ipynb_checkpoints/`, `__pycache__`, `titanic.csv`, `iris.csv`, and `telco.csv`, `*.csv`. Add and commit your `.gitignore` file before moving forward.\n",
    "    \n",
    "3. Now that you are 100% sure that your `.gitignore` file lists `env.py`, create or copy your `env.py` file inside of `classification-exercises`. Running `git status` should show that git is ignoring this file.\n",
    "\n",
    "4. In a jupyter notebook, `classification_exercises.ipynb`, use a python module (pydata or seaborn datasets) containing datasets as a source from the iris data. Create a pandas dataframe, `df_iris`, from this data.\n",
    "\n",
    "    - print the first 3 rows\n",
    "    - print the number of rows and columns (shape)\n",
    "    - print the column names\n",
    "    - print the data type of each column\n",
    "    - print the summary statistics for each of the numeric variables. Would you\n",
    "      recommend rescaling the data based on these statistics?\n",
    "\n",
    "\n",
    "5. Read the `Table1_CustDetails` table from the `Excel_Exercises.xlsx` file into a\n",
    "   dataframe named `df_excel`.\n",
    "\n",
    "    - assign the first 100 rows to a new dataframe, `df_excel_sample`\n",
    "    - print the number of rows of your original dataframe\n",
    "    - print the first 5 column names\n",
    "    - print the column names that have a data type of `object`\n",
    "    - compute the range for each of the numeric variables.\n",
    "\n",
    "\n",
    "6. Read the data from [this google sheet](https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/edit?usp=sharing) into a dataframe, `df_google`\n",
    "\n",
    "    - print the first 3 rows\n",
    "    - print the number of rows and columns\n",
    "    - print the column names\n",
    "    - print the data type of each column\n",
    "    - print the summary statistics for each of the numeric variables\n",
    "    - print the unique values for each of your categorical variables\n",
    "\n",
    "\n",
    "Make a new python module, `acquire.py` to hold the following data aquisition functions:\n",
    "\n",
    "7. Make a function named `get_titanic_data` that returns the titanic data from the codeup data science database as a pandas data frame. Obtain your data from the _Codeup Data Science Database_. \n",
    "\n",
    "\n",
    "8. Make a function named `get_iris_data` that returns the data from the `iris_db` on the codeup data science database as a pandas data frame. The returned data frame should include the actual name of the species in addition to the `species_id`s. Obtain your data from the _Codeup Data Science Database_. \n",
    "\n",
    "9. Make a function named `get_telco_data` that returns the data from the `telco_churn` database in SQL. In your SQL, be sure to join all 4 tables together, so that the resulting dataframe contains all the contract, payment, and internet service options. Obtain your data from the _Codeup Data Science Database_. \n",
    "\n",
    "10. Once you've got your `get_titanic_data`, `get_iris_data`, and `get_telco_data` functions written, now it's time to add caching to them. To do this, edit the beginning of the function to check for the local filename of `telco.csv`, `titanic.csv`, or `iris.csv`. If they exist, use the .csv file. If the file doesn't exist, then produce the SQL and pandas necessary to create a dataframe, then write the dataframe to a .csv file with the appropriate name. \n",
    "\n",
    "__Be sure to add env.py, titanic.csv, iris.csv, and telco.csv to your .gitignore file__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
